{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "import gc\n",
    "from keras.preprocessing.image import image,load_img,img_to_array\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense,Conv2D,MaxPool2D,Flatten,Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing set\n",
      "Cannot process .DS.png\n",
      "500 images converted\n",
      "1000 images converted\n",
      "Conversion Completed\n"
     ]
    }
   ],
   "source": [
    "#processing the image and converting them into numbers\n",
    "#getting all images\n",
    "#train_pics = image.list_pictures(\"train/\")\n",
    "test_pics = image.list_pictures(\"test/\")\n",
    "\n",
    "def image_resize(list,folder):\n",
    "    #convert all to the same dimensions of 100*100\n",
    "    os.mkdir(folder)\n",
    "    i = 0\n",
    "    for im in list:\n",
    "        #get old name\n",
    "        name = im.split(\"/\")[1]\n",
    "        try:\n",
    "            #read the image\n",
    "            z = load_img(im)\n",
    "            #resize the image\n",
    "            z = z.resize([64,64])\n",
    "            #save the image\n",
    "            z.save(folder+name)\n",
    "            i += 1\n",
    "            if (i %500 == 0 ):\n",
    "                print(\"{} images converted\".format(i))\n",
    "        except:\n",
    "            print(\"Cannot process {}\".format(name))\n",
    "    print(\"Conversion Completed\")\n",
    "#print(\"training set\")\n",
    "#image_resize(train_pics,\"cotrain/\")\n",
    "print(\"testing set\")\n",
    "image_resize(test_pics,\"cotest/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set\n",
      "500 images converted\n",
      "1000 images converted\n",
      "Conversion is Completed\n"
     ]
    }
   ],
   "source": [
    "#train = image.list_pictures(\"cotrain/\")\n",
    "test = image.list_pictures(\"cotest/\")\n",
    "#instatiate the test and train images\n",
    "#d1 = np.zeros([len(train),64,64,3])\n",
    "d2 = np.zeros([len(test),64,64,3])\n",
    "def convert_to_numpy(file,list):\n",
    "    i = 0\n",
    "    for im in file:\n",
    "        #read the image\n",
    "        try:\n",
    "            z = load_img(im)\n",
    "            #convert into images and replace in the instatiated datasets above\n",
    "            list[i] = img_to_array(z)\n",
    "            i += 1\n",
    "            if (i%500==0):\n",
    "                print(\"{} images converted\".format(i))\n",
    "        except:\n",
    "            print(\"Cannot process image {}\".format(im))\n",
    "    print (\"Conversion is Completed\")\n",
    "#print(\"Training Set\")\n",
    "#convert_to_numpy(train,d1)\n",
    "print(\"Testing Set\")\n",
    "convert_to_numpy(test,d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #normalizing the data\n",
    "d1 = (d1 -128)/ 128\n",
    "d2 = (d2 - 128)/ 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the id for images accordingly\n",
    "img_id = []\n",
    "img_id2 = []\n",
    "for dir in train:\n",
    "    img_id.append(dir.split(\"/\")[1].split(\".\")[0])\n",
    "img_id_df = pd.Series(img_id,name='img_id').astype(\"int64\")\n",
    "\n",
    "for dir in test:\n",
    "    img_id.append(dir.split(\"/\")[1].split(\".\")[0])\n",
    "img_id_df2 = pd.Series(img_id2,name='img_id').astype(\"int64\")\n",
    "\n",
    "#reshape your image and join\n",
    "dt1 = d1.reshape([d1.shape[0],64*64*3])\n",
    "dt2 = d2.reshape([d2.shape[0],64*64*3])\n",
    "dt_df = pd.DataFrame(dt1)\n",
    "dt_df2 = pd.DataFrame(dt2)\n",
    "\n",
    "del dt1\n",
    "del dt2\n",
    "del img_id\n",
    "del img_id2\n",
    "\n",
    "#joining the dataframe together \n",
    "dt = dt_df.join(img_id_df)\n",
    "dt2 = dt_df2.join(img_id_df2)\n",
    "\n",
    "del dt_df\n",
    "del dt_df2\n",
    "del img_id_df\n",
    "del img_id_df2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12279</th>\n",
       "      <th>12280</th>\n",
       "      <th>12281</th>\n",
       "      <th>12282</th>\n",
       "      <th>12283</th>\n",
       "      <th>12284</th>\n",
       "      <th>12285</th>\n",
       "      <th>12286</th>\n",
       "      <th>12287</th>\n",
       "      <th>img_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.632812</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>0.695312</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.710938</td>\n",
       "      <td>0.632812</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.523438</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.289062</td>\n",
       "      <td>-0.312500</td>\n",
       "      <td>-0.367188</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.820312</td>\n",
       "      <td>0.617188</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.421875</td>\n",
       "      <td>-0.453125</td>\n",
       "      <td>-0.546875</td>\n",
       "      <td>-0.421875</td>\n",
       "      <td>-0.437500</td>\n",
       "      <td>-0.554688</td>\n",
       "      <td>-0.429688</td>\n",
       "      <td>-0.460938</td>\n",
       "      <td>-0.546875</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.398438</td>\n",
       "      <td>0.257812</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.335938</td>\n",
       "      <td>0.195312</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>-0.109375</td>\n",
       "      <td>0.328125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.203125</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.351562</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>0.226562</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>0.289062</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.304688</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.320312</td>\n",
       "      <td>0.179688</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.289062</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>0.445312</td>\n",
       "      <td>0.445312</td>\n",
       "      <td>0.195312</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>-0.734375</td>\n",
       "      <td>-0.757812</td>\n",
       "      <td>-0.757812</td>\n",
       "      <td>-0.742188</td>\n",
       "      <td>-0.734375</td>\n",
       "      <td>-0.734375</td>\n",
       "      <td>-0.718750</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12289 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.687500  0.632812  0.539062  0.695312  0.640625  0.554688  0.710938   \n",
       "1 -0.289062 -0.312500 -0.367188  0.609375  0.687500  0.820312  0.617188   \n",
       "2  0.398438  0.257812  0.085938  0.335938  0.195312  0.023438  0.218750   \n",
       "3  0.546875  0.421875  0.226562  0.531250  0.406250  0.210938  0.554688   \n",
       "4  0.406250  0.312500  0.289062  0.531250  0.429688  0.421875  0.539062   \n",
       "\n",
       "          7         8         9   ...       12279     12280     12281  \\\n",
       "0  0.632812  0.554688  0.718750   ...    0.578125  0.500000  0.429688   \n",
       "1  0.703125  0.781250  0.609375   ...   -0.421875 -0.453125 -0.546875   \n",
       "2  0.078125 -0.109375  0.328125   ...    0.375000  0.234375  0.031250   \n",
       "3  0.429688  0.234375  0.515625   ...    0.421875  0.289062  0.140625   \n",
       "4  0.445312  0.445312  0.195312   ...   -0.750000 -0.750000 -0.734375   \n",
       "\n",
       "      12282     12283     12284     12285     12286     12287  img_id  \n",
       "0  0.546875  0.468750  0.390625  0.523438  0.437500  0.390625     NaN  \n",
       "1 -0.421875 -0.437500 -0.554688 -0.429688 -0.460938 -0.546875     NaN  \n",
       "2  0.343750  0.203125  0.015625  0.351562  0.210938  0.023438     NaN  \n",
       "3  0.437500  0.304688  0.156250  0.453125  0.320312  0.179688     NaN  \n",
       "4 -0.757812 -0.757812 -0.742188 -0.734375 -0.734375 -0.718750     NaN  \n",
       "\n",
       "[5 rows x 12289 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the labels datasets\n",
    "id_df = pd.read_csv(\"train_labels.csv\")\n",
    "id_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "\n",
    "#merge the two datasets for proper labeling\n",
    "final_train_df = dt.merge(id_df,on=\"img_id\")\n",
    "\n",
    "#---------------------------------------------------------------------#\n",
    "\n",
    "#reading the labels datasets\n",
    "id_df2 = pd.read_csv(\"test_ids.csv\")\n",
    "\n",
    "#merge the two datasets for proper labeling\n",
    "final_test_df = dt2.merge(id_df2,on=\"img_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_train_df = pd.read_csv(\"training.csv\")\n",
    "\n",
    "X = final_train_df.drop(['fuel','parking','restaurant','transport','img_id'],axis=1)\n",
    "y = final_train_df[['fuel','parking','restaurant','transport']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting back to numpy and reshaping\n",
    "Xtrain = X_train.values.reshape([len(X_train),64,64,3])\n",
    "Xtest = X_test.values.reshape([len(X_test),64,64,3])\n",
    "ytrain = y_train.values\n",
    "ytest = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #an MlP model\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Dense(256,activation=\"relu\"))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(128,activation=\"relu\"))\n",
    "# model.add(Dropout(0.25))\n",
    "# #model.add(Dense(32,activation=\"relu\"))\n",
    "# #model.add(Dropout(0.5))\n",
    "# model.add(Dense(4,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#structure of your convolution layer\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32,(3,3),activation=\"relu\",input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(32,(5,5),activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation=\"relu\"))\n",
    "model.add(Dropout(rate=0.75))\n",
    "model.add(Dense(1024,activation=\"relu\"))\n",
    "model.add(Dropout(0.75))\n",
    "model.add(Dense(4,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",loss=\"mse\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(Xtrain,ytrain,batch_size=128,epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluating is below this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = accuracy_score(ytest.argmax(1),pred.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.568407960199005"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.5 model approximately\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "using a rescale of 32*32\n",
    "\n",
    "#structure of your convolution layer\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16,(3,3),activation=\"relu\",input_shape=(32,32,3)))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(32,(5,5),activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation=\"relu\"))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(4,activation=\"softmax\"))\n",
    "\n",
    "a catcrossent as loss function\n",
    "\n",
    "and an adam opt..\n",
    "\n",
    "\"\"\"\"\n",
    "\"\"\"\n",
    "this performed better on the test set than on the training sets\n",
    "using dropouts of 75% on the mlp\n",
    "\n",
    "#structure of your convolution layer\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16,(3,3),activation=\"relu\",input_shape=(32,32,3)))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(16,(5,5),activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation=\"relu\"))\n",
    "model.add(Dropout(rate=0.75))\n",
    "model.add(Dense(512,activation=\"relu\"))\n",
    "model.add(Dropout(0.75))\n",
    "model.add(Dense(4,activation=\"softmax\"))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "55% on the testing set\n",
    "\n",
    "#structure of your convolution layer\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32,(3,3),activation=\"relu\",input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(32,(5,5),activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation=\"relu\"))\n",
    "model.add(Dropout(rate=0.75))\n",
    "model.add(Dense(1024,activation=\"relu\"))\n",
    "model.add(Dropout(0.75))\n",
    "model.add(Dense(4,activation=\"softmax\"))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#things to note based on the tuning\n",
    "\n",
    "#1)More nodes in a layer is better than a deeper layer\n",
    "\n",
    "#2)adam is the best optimizer\n",
    "\n",
    "#3)with probably categorical_crossentropy as the loss function\n",
    "\n",
    "#4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
